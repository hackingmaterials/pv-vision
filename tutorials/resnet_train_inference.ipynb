{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np \n",
    "import cv2 as cv\n",
    "import os \n",
    "from pathlib import Path\n",
    "from imutils.paths import list_images\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "import pv_vision.defective_cell_detection.model.cnn_train_val as cnn"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "Folder structure is: \\\n",
    ". \\\n",
    "|-- rf_train_inference.ipynb \\\n",
    "|-- segmented_cells \\\n",
    "....|-- train \\\n",
    "....|...|-- class 1 \\\n",
    "....|...|-- class 2 \\\n",
    "....|...|-- class ... \\\n",
    "....|...\\`-- class n \\\n",
    "....|-- val \\\n",
    "....|...|-- class 1 \\\n",
    "....|...|-- class 2 \\\n",
    "....|...|-- class ... \\\n",
    "....|...\\`-- class n \\\n",
    "....\\`-- test \\\n",
    "........|-- class 1 \\\n",
    "........|-- class 2 \\\n",
    "........|-- class ... \\\n",
    "........`-- class n \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "im_train_dir = Path('segmented_cells/train')\n",
    "im_val_dir = Path('segmented_cells/val')\n",
    "im_test_dir = Path('segmented_cells/test')\n",
    "\n",
    "labels_train = []\n",
    "images_train = []\n",
    "\n",
    "labels_val = []\n",
    "images_val = []\n",
    "names_val = []\n",
    "\n",
    "labels_test = []\n",
    "images_test = []\n",
    "names_test= []\n",
    "\n",
    "for im_path in tqdm(list(list_images(im_train_dir))):\n",
    "    images_train.append(cv.imread(im_path))\n",
    "    labels_train.append(im_path.split('/')[-2])\n",
    "\n",
    "for im_path in tqdm(list(list_images(im_val_dir))):\n",
    "    images_val.append(cv.imread(im_path))\n",
    "    labels_val.append(im_path.split('/')[-2])\n",
    "    names_val.append(os.path.splitext(os.path.split(im_path)[-1])[0])\n",
    "\n",
    "for im_path in tqdm(list(list_images(im_test_dir))):\n",
    "    images_test.append(cv.imread(im_path))\n",
    "    labels_test.append(im_path.split('/')[-2])\n",
    "    names_test.append(os.path.splitext(os.path.split(im_path)[-1])[0])\n",
    "    \n",
    "images_train = np.array(images_train)\n",
    "images_val = np.array(images_val)\n",
    "images_test = np.array(images_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(labels_train)\n",
    "y_val = le.transform(labels_val)\n",
    "y_test = le.transform(labels_test)\n",
    "\n",
    "# assign the device to run the code on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define dataset and dataloader\n",
    "solar_transform_train = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    cnn.OneRotationTransform([0,180]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, )) #grayscale only\n",
    "]) \n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) \n",
    "\n",
    "solar_transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "]) \n",
    "\n",
    "# build dataloader\n",
    "solar_train = cnn.SolarDataset(images_train, y_train, transform=solar_transform_train)#, transform2=solar_transform_val, inx_aug=[0, 2, 3, 4]) # determine whether to use aug\n",
    "solar_val = cnn.SolarDataset(images_val, y_val, transform=solar_transform_val)\n",
    "solar_test = cnn.SolarDataset(images_test, y_test, transform=solar_transform_val)\n",
    "\n",
    "trainloader = DataLoader(solar_train, batch_size=128, shuffle=True)\n",
    "valloader = DataLoader(solar_val, batch_size=128, shuffle=False)\n",
    "testloader = DataLoader(solar_test, batch_size=128, shuffle=False)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train the model (ResNet18)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize the model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 5)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# train\n",
    "model_fit_wts, loss_acc = cnn.train_model(model, trainloader, valloader, solar_train, solar_val, criterion, optimizer, lr_scheduler, device, num_epochs=20)\n",
    "\n",
    "model_path = Path('cnn')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "with open(model_path/'resnet18_loss_acc.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_acc, f)\n",
    "for metric_name, wt in model_fit_wts.items():\n",
    "    ### save model\n",
    "    models_subpath = model_path/metric_name\n",
    "    os.makedirs(models_subpath, exist_ok=True)\n",
    "    torch.save(wt, models_subpath/'resnet18_model.pth')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load model\n",
    "model_fit = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_fit.fc.in_features\n",
    "model_fit.fc = nn.Linear(num_ftrs, 5)\n",
    "model_fit.load_state_dict(torch.load(model_path/'best_loss'/'resnet18_model.pth', map_location=\"cuda:0\")) \n",
    "model_fit.to(device)\n",
    "model_fit.eval();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build predloader. \n",
    "# When applying the model to new dataset, the labels are unknown.\n",
    "solar_test = cnn.PredDataset(images_test, transform=solar_transform_val)\n",
    "\n",
    "predloader = DataLoader(solar_test, batch_size=128, shuffle=False)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make prediction\n",
    "pred_test, prob_test = cnn.predict_test(predloader, model_fit, device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save the prediction\n",
    "le = LabelEncoder()\n",
    "le.fit(['crack', 'intact', 'intra', 'oxygen', 'solder'])\n",
    "\n",
    "with open(Path('cnn')/'results'/'resnet18_predicted.pkl', 'wb') as f:\n",
    "    pickle.dump({'name': np.array(names_test), \n",
    "                'defects_pred': le.inverse_transform(pred_test),\n",
    "                'y_pred': np.array(pred_test)}, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('pytorch_env': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f4080700e573899fc63e7d3787ad8d019467046cf1e1a46e1ffb125d1e176d46"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}