{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from imutils.paths import list_images\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pv_vision.nn import ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will put this method into util in the future\n",
    "class SolarDataset(VisionDataset):\n",
    "    \"\"\"A dataset directly read images and masks from folder.    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 image_folder, \n",
    "                 mask_folder,\n",
    "                 transforms,\n",
    "                 mode = \"train\",\n",
    "                 random_seed=42):\n",
    "        super().__init__(root, transforms)\n",
    "        self.image_path = Path(self.root) / image_folder\n",
    "        self.mask_path = Path(self.root) / mask_folder\n",
    "\n",
    "        if not os.path.exists(self.image_path):\n",
    "            raise OSError(f\"{self.image_path} not found.\")\n",
    "\n",
    "        if not os.path.exists(self.mask_path):\n",
    "            raise OSError(f\"{self.mask_path} not found.\")\n",
    "\n",
    "        self.image_list = sorted(list(list_images(self.image_path)))\n",
    "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
    "\n",
    "        self.image_list = np.array(self.image_list)\n",
    "        self.mask_list = np.array(self.mask_list)\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "        index = np.arange(len(self.image_list))\n",
    "        np.random.shuffle(index)\n",
    "        self.image_list = self.image_list[index]\n",
    "        self.mask_list = self.mask_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getname__(self, index):\n",
    "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
    "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
    "\n",
    "        if image_name == mask_name:\n",
    "            return image_name\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __getraw__(self, index):\n",
    "        if not self.__getname__(index):\n",
    "            raise ValueError(\"{}: Image doesn't match with mask\".format(os.path.split(self.image_list[index])[-1]))\n",
    "        image = Image.open(self.image_list[index])\n",
    "        mask = Image.open(self.mask_list[index]).convert('L')\n",
    "        mask = np.array(mask)\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, mask = self.__getraw__(index)\n",
    "        image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will put into utils in the future\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        \"\"\"\n",
    "        transforms: a list of transform\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        image: input image\n",
    "        target: input mask\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class FixResize:\n",
    "    # UNet requires input size to be multiple of 16\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Transform the image to tensor. Scale the image to [0,1] float32.\n",
    "    Transform the mask to tensor.\n",
    "    \"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = transforms.ToTensor()(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "class PILToTensor:\n",
    "    \"\"\"Transform the image to tensor. Keep raw type.\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.pil_to_tensor(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images and lables are only for tutorial demosntration.\n",
    "# The complete data set we used for model development can be found here:\n",
    "# https://datahub.duramat.org/dataset/00b29daf-239c-47b6-bd96-bfb0875179a8/resource/c6626a05-e82f-4732-ade9-ec5441b83e46/download/crack_segmentation.zip\n",
    "\n",
    "root = Path('../examples/crack_segmentation/img_label_for_training')\n",
    "transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
    "\n",
    "trainset = SolarDataset(root, image_folder=\"train/img\", \n",
    "        mask_folder=\"train/ann\", transforms=transformers)\n",
    "\n",
    "valset = SolarDataset(root, image_folder=\"val/img\", \n",
    "        mask_folder=\"val/ann\", transforms=transformers)\n",
    "\n",
    "testset = SolarDataset(root, image_folder=\"testset/img\", \n",
    "        mask_folder=\"testset/ann\", transforms=transformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepLab_pretrained(num_classes):\n",
    "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
    "    return deeplab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DataParallel(DeepLab_pretrained(5))\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize modelhandler\n",
    "# The output is stored in the output folder\n",
    "modelhandler = ModelHandler(\n",
    "    model=model,\n",
    "    model_output='out',\n",
    "    train_dataset=trainset,\n",
    "    val_dataset=valset,\n",
    "    test_dataset=testset,\n",
    "    batch_size_train=32,\n",
    "    batch_size_val=32,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    num_epochs=10,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    save_dir='checkpoints',\n",
    "    save_name='deeplab.pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:16<00:00,  2.45s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4173 (train) | 4.9578 (val)\n",
      "Epoch 2 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:17<00:00,  2.48s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1789 (train) | 0.1916 (val)\n",
      "Epoch 3 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:18<00:00,  2.53s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1192 (train) | 0.1170 (val)\n",
      "Epoch 4 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:19<00:00,  2.57s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1049 (train) | 0.1077 (val)\n",
      "Epoch 5 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:19<00:00,  2.57s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0942 (train) | 0.0964 (val)\n",
      "Epoch 6 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:19<00:00,  2.58s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0882 (train) | 0.0882 (val)\n",
      "Epoch 7 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:20<00:00,  2.58s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0849 (train) | 0.0861 (val)\n",
      "Epoch 8 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:19<00:00,  2.58s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0839 (train) | 0.0836 (val)\n",
      "Epoch 9 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:19<00:00,  2.57s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0826 (train) | 0.0833 (val)\n",
      "Epoch 10 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:20<00:00,  2.58s/it]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0800 (train) | 0.0824 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': {'loss': [0.4173066987054338,\n",
       "   0.17885671862604197,\n",
       "   0.1192198520825233,\n",
       "   0.1049448441514753,\n",
       "   0.09418112763161522,\n",
       "   0.08817903688292444,\n",
       "   0.0848550075059565,\n",
       "   0.08386343555072698,\n",
       "   0.08257343704379144,\n",
       "   0.07998113717439244]},\n",
       " 'val': {'loss': [4.957792570424634,\n",
       "   0.19160857034284015,\n",
       "   0.11703312431657037,\n",
       "   0.10774861033572707,\n",
       "   0.09641222534484642,\n",
       "   0.08819531077562376,\n",
       "   0.08607308133396992,\n",
       "   0.08364118046538774,\n",
       "   0.08325028124936791,\n",
       "   0.08244443407585454]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model. Note that this tutorial only runs 10 epochs. \n",
    "# It may take longer training iteration in real situation.\n",
    "modelhandler.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize training process\n",
    "modelhandler.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the minimum loss in validation, which is the best model\n",
    "np.argmin(modelhandler.running_record['val']['loss'])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load the best model and check its performance on testing set\n",
    "modelhandler.load_model('checkpoints/epoch_10/deeplab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mode\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = modelhandler.test_model(cache_output='testset_output')\n",
    "# Not bad. The testing score is similar to validation score\n",
    "# the output of the model is stored in self.cache['testset_output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-pv-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
